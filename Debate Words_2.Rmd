---
title: "BIAS AND CONTEXT IN PRESIDENTIAL DEBATE TEXTS"
author: "Winston Saunders"
date: "December 5, 2015"
output: 
    html_document:
        theme: united
---

###SUMMARY
 
Perceptions of priorities are influenced, in part, by how frequently we hear about an issue and also the context in which we hear it. Can we detect systematic differences in language reflecting political priorities and biases?
Here, using standard NLP (Natural Lanuguage Processing) techniques, I explore this question looking for differences in the texts from recent Republican and Democratic presidential debates. 
Key findings are:   
1. "Wordcloud" visualization reveals some stylistic differences between candidates but no clarity on specifi postiions. .  
2. Word-frequencies of selected "key-words" suggest positions differences. A _z-statistic_ can be used to highlight signficant differences between candidates.   
3. Initial results for bigram tokenization reveal differences some differences in key-word context.   


###DATA SOURCES AND METHODS
The text of the presidential debates are downloaded from the [UCSB Presidency Project](http://www.presidency.ucsb.edu/debates.php). Transcripts were pasted into Apple Pages and stored as unformatted .txt files. From that point all processing is done with __R__ using capabilities of {tm} and associated libraries.  


###CANDIDATE WORD-CLOUDS

A quick and visually apprealing method to compare texts is word-frequency analysis using the {wordcloud} package in __R__. 
Not surprisingly, word choices vary between candidates. However, there are also some striking similarities.  

```{r, echo=FALSE, warning=FALSE, message=FALSE}


load_debate_text <- function(file_name){

## GET THE DATA
    
    ## data are the raw text files downloaded from the UCSB website.
    
    directory <- "/Users/winstonsaunders/Documents/Presidential_Debates_2015/"
    mydata <- read.table(paste0(directory, file_name), header=FALSE, sep="\n", stringsAsFactors = FALSE, quote = "")

    ## This is what a sample of the the raw data looks like
    ## mydata[100:105,1]
        # [1] "QUINTANILLA: Hold it. We'll cut it back to..."
        # [2] "QUICK: Dr. Carson, let's talk about taxes."
        # [3] "You have a flat tax plan of 10 percent flat taxes..."
        # [4] "If you were to took a 10 percent tax..."
        # [5] "So what analysis got you to the point where you think this will work?"
        # [6] "CARSON: Well, first of all..."

## ADD COLUMNS OF NUMBERS AND SEPARATE TEXT FROM CANDIDATE NAMES
    ## Add number column
    #mydata$n<-1:nrow(mydata)
    
    ## get rid of "..."
    #mydata$V1<-gsub("...", "", mydata$V1)
    
    ## This regex gets rid of all but the capitalized names
    mydata$name <- gsub(":.+|^[A-Z][^A-Z].+", "", mydata$V1 )

    ## Fill in the blank rows
    for (i in 2: nrow(mydata)){
        if (mydata$name[i] == "") mydata$name[i] <- mydata$name[i-1]
    }

    ## CREATE COLUMN OF DEBATE TEXT AND CLEAN UP BLANK SPACES
    mydata$text <- gsub (".", "", mydata$V1, fixed=TRUE)
    mydata$text <- gsub ("'", "", mydata$text)
    mydata$text <- gsub ("=", "", mydata$text)
    mydata$text <- gsub ("[A-Z]{2,}: ", "", mydata$text)
    mydata$text <- tolower(mydata$text)
    mydata$text <- gsub ("   ", " ", mydata$text)
    mydata$text <- gsub ("  ", " ", mydata$text)
    ##stem a few words
    mydata$text <- gsub ("taxes", "tax", mydata$text)
    mydata$text <- gsub ("guns", "gun", mydata$text)
    mydata$text <- gsub ("veterans", "veteran", mydata$text)
    mydata$text <- gsub ("terrorists", "terror", mydata$text)
    mydata$text <- gsub ("terrorism", "terror", mydata$text)
    
    ## some unused text filters. 
    #mydata$text <- gsub ("streets", "street", mydata$text)
    #mydata$text <- gsub ("walls", "wall", mydata$text)
    #mydata$text <- gsub ("womens", "women", mydata$text)
    
    #mydata$text <- gsub ("americans", "american", mydata$text)
    #mydata$text <- gsub ("american", "america", mydata$text)

    
    
    
    ## the data frame now contains four columns which look like this...
    
        #   50 Our greatest days lie ahead...
        #   51 QUINTANILLA: Mr. Trump?
        #   52 TRUMP: I think maybe ...[laughter]
        #     n        name
        # 50 50       RUBIO
        # 51 51 QUINTANILLA
        # 52 52       TRUMP
        # text
        # 50  Our greatest days lie ahead ....
        # 51  Mr. Trump?
        # 52  I think maybe my greatest weakness ... [laughter]
    
    return(mydata)
}

```

```{r echo=FALSE, warning=FALSE, message=FALSE}

candidate_text <- function(candidate, mydata){
    ## 
    ## GET CANDIDATE TEXT
    ##
    ## Assumes load_debate_text has been run and the debate text is stored in "mydata"
    ## creates a text list
    
  
    
    ## filter for candidate
    text<-mydata$text[mydata$name==candidate]
    #text<-paste(text, collapse = " ")

    return(text)


}
```


```{r echo=25:29, warning=FALSE, message=FALSE}

    ##
    ## TEXT_TC
    ##
    ## this function returns a text corpus
    ##
    ##

    library(tm)

    library(RWeka)

    text_tc <- function(mydata){
    ## 
    ## 
    ##
    ## Assumes text is stored in "mydata"
    ## creates a Corpus from the text
    ## filters some words out 

    require(tm)
    require(SnowballC)

    t_c <- Corpus(VectorSource(text))
    t_c <- tm_map(t_c, content_transformer(tolower))
    t_c <- tm_map(t_c, removePunctuation)
    t_c <- tm_map(t_c, removeNumbers)
    t_c <- tm_map(t_c, removeWords, stopwords("english"))
    t_c <- tm_map(t_c, removeWords, c("applause", "thats", "laughter", "dont", "back", "can", "get", "cant", "come", "big", "inaudible", "dont", "back", "can", "get"))

    return(t_c)   
    
} 

```


```{r echo=FALSE, warning=FALSE, message=FALSE}

    library(tm)
    library(RWeka)
    library(SnowballC)

candidate_text_tc <- function(candidate, mydata){
    ## 
    ## GET CANDIDATE DATA
    ##
    ## Assumes load_debate_text has been run and the debate text is stored in "mydata"
    ## creates a Corpus from the candidate text
    
  
    
    ## filter for candidate
    text<-mydata$text[mydata$name==candidate]
    text<-paste(text, collapse = " ")

    require(tm)
    require(SnowballC)

    t_c <- Corpus(VectorSource(text))
    t_c <- tm_map(t_c, content_transformer(tolower))
    t_c <- tm_map(t_c, removePunctuation)
    t_c <- tm_map(t_c, removeNumbers)
    t_c <- tm_map(t_c, removeWords, stopwords("english"))
    t_c <- tm_map(t_c, removeWords, c("applause", "thats", "laughter", "dont", "back", "can", "get", "cant", "come", "big", "inaudible", "dont", "back", "can", "get"))

    #t_c <- tm_map(t_c, stemDocument)
    
    return(t_c)   
    
} 

```

```{r echo=FALSE, warning=FALSE, message=FALSE}

library(wordcloud)
library(RColorBrewer)

 color_map2<-c("#00003B","#041851","#4E0812" ,"#051E65", "#650A16", "#103374",  "#9F1A2D", "#482B25")

c_wordcloud <- function(t_c){
    
    ## 
    ## CREATE WORD CLOUD
    ##
    ## Assumes a text Corpus has been created
    ## 
    
    set.seed(8675309)
    
   
    color_map2<-c("#00003B","#041851","#4E0812" ,"#051E65", "#650A16", "#103374",  "#9F1A2D", "#482B25")
    
    wordcloud(t_c, scale=c(4,0.4), max.words=150, min.freq=5, random.order=FALSE, rot.per=0.2, use.r.layout=FALSE, colors=color_map2)

    

}

```



```{r, echo=FALSE, warning=FALSE, message=FALSE}

    ## GET ALL REPUB DEBATES
     file_name<-"Republican Candidates Debate in Boulder Colorado October 28 2015.txt"
     r_oct<-load_debate_text(file_name)
     #r_oct$date<-"Oct"
     file_name<-"Republican Candidates Debate in Milwaukee Wisconsin November 10 2015.txt"
     r_nov<-load_debate_text(file_name)
     #r_nov$date<-"Nov"
    
    r_all = rbind(r_oct, r_nov)
        ## slim down data a bit
        ## reduce data to about 30%
#         r_all<-r_all[grepl("RUBIO", r_all$name)|
#                          grepl("CARSON", r_all$name)|
#                          grepl("TRUMP", r_all$name)|
#                          grepl("CRUZ", r_all$name)|
#                          grepl("HUCKABEE", r_all$name)|
#                          grepl("FIORINA", r_all$name), ]
    
    
    
    ## GET ALL DEMO DEBATES
     file_name<-"Democratic Candidates Debate in Las Vegas Nevada October 13 2015 .txt"
     d_oct<-load_debate_text(file_name)
     file_name<-"Democratic Candidates Debate in Des Monies Iowa November 14 2015 .txt"
     d_nov<-load_debate_text(file_name)
    
    d_all = rbind(d_oct, d_nov)
    
    ## CREATE TCs FOR EACH CANDIDATE
    trump_all<-candidate_text_tc("TRUMP",r_all)
    rubio_all<-candidate_text_tc("RUBIO",r_all)
    fiorina_all<-candidate_text_tc("FIORINA",r_all)
    carson_all<-candidate_text_tc("CARSON",r_all)
    cruz_all<-candidate_text_tc("CRUZ",r_all)
    huckabee_all<-candidate_text_tc("HUCKABEE",r_all)
    bush_all<-candidate_text_tc("BUSH",r_all)

    clinton_all<-candidate_text_tc("CLINTON",d_all)
    sanders_all<-candidate_text_tc("SANDERS",d_all)
    
    
    
```






<style>
  .col2 {
    columns: 2 300px;         /* number of columns and width in pixels*/
    -webkit-columns: 2 300px; /* chrome, safari */
    -moz-columns: 2 300px;    /* firefox */
  }
  .col3 {
    columns: 3 200px;
    -webkit-columns: 3 200px;
    -moz-columns: 3 200px;
  }
</style>




Let's first compare the word clouds of candidates using the {wordcloud} package.   

####TRUMP V. SANDERS

Bernie's word cloud is larger than Donald's, due to having spoken more total words. (There were three major candidates at the Democratic debate and ten at Republican). What I find most surprising is the _similarity_ of the clouds; words like _"people"_, _"country"_, and _"going"_ are common to both. Despite strong differences in policy, word clouds reveal little about them.  

<div class="col2">


```{r, echo=TRUE, message=FALSE, fig.align="center", results="asis"}
c_wordcloud(trump_all)
c_wordcloud(sanders_all)
```

</div>

####HILARY V. CARLY

In this case the word clouds couldn't be more different. Hilary's emphasis on _"think"_ and _"people"_ differs remarkably from Carly's emphasis of _"government"_.  

<div class="col2">


```{r, echo=TRUE, message=FALSE, fig.align="center", results="asis"}
c_wordcloud(clinton_all)
c_wordcloud(fiorina_all)
```

</div>

####CRUZ V. HUCKABEE

Ted Cruz's wordcloud emphasize technicalities, like _"taxes"_ and _"washington"_, while that of Mike Huckabee, a former minister, seems a mix of Mr. Trump's and Ms. Fiorina's.

<div class="col2">


```{r, echo=TRUE, message=FALSE, fig.align="center", results="asis"}
c_wordcloud(cruz_all)
c_wordcloud(huckabee_all)
```

</div>


###STAYING ON MESSAGE: COMPARING DEBATES

We can also split the text by specific debate. Since the debates cover different topics and questions, one might expect to see this reflected in the text of the separate dialogues. What's surprising here is how comparable the language of each candidate is between the debates. 

<div class="col2">


```{r, echo=TRUE, message=FALSE, fig.align="center", results="asis"}
c_wordcloud(candidate_text_tc("TRUMP", r_oct))
c_wordcloud(candidate_text_tc("TRUMP", r_nov))
```
</div>

<div class="col2">


```{r, echo=TRUE, message=FALSE, fig.align="center", results="asis"}
c_wordcloud(candidate_text_tc("SANDERS", d_oct))
c_wordcloud(candidate_text_tc("SANDERS", d_nov))
```
</div>


###WORD FREQUENCY

We can check word frequency directly by tokenizing the text and counting single words. 


<style>
tr:hover {background-color: #F5A5A5}
table { 
    width: 80%;
    display: table;
    border-collapse: collapse;
    border-spacing: 18px;
    border-color: #AAAAFF;
    background-color: #A5A1F5;
    padding: 2px;
    font: 16px arial, sans-serif;
}
th, td{
    text-align: center;
}
</style>


```{r, echo=FALSE}

## Create Term_Document_Matrices

TDM_trump <- TermDocumentMatrix(trump_all)
TDM_rubio <- TermDocumentMatrix(rubio_all)
TDM_fiorina <- TermDocumentMatrix(fiorina_all)
TDM_carson <- TermDocumentMatrix(carson_all)
TDM_cruz <- TermDocumentMatrix(cruz_all)
TDM_huckabee <- TermDocumentMatrix(huckabee_all)
TDM_bush <- TermDocumentMatrix(bush_all)

TDM_clinton <- TermDocumentMatrix(clinton_all)
TDM_sanders <- TermDocumentMatrix(sanders_all)

```



```{r, echo=FALSE, results='asis', fig.align='center'}

library(xtable)

## Create table of frequent terms
## note: The Frequent Term utility does not rank by order. It produces a character vector of terms in x which occur more or equal often than lowfreq times and less or equal often than highfreq times.

# row_names <- c("Trump", "Sanders", "Clinton", "Fiorina", "Cruz", "Rubio" )
# col_names <- c("most frequent", "Second", "Third", "Fourth", "Fifth")

# word_mat<-matrix(
#     c(findFreqTerms(TDM_trump, 30)[1:5],
#       findFreqTerms(TDM_sanders, 30)[1:5],
#       findFreqTerms(TDM_clinton, 30)[1:5],
#       findFreqTerms(TDM_fiorina, 10)[1:5],
#       findFreqTerms(TDM_cruz, 10)[1:5],
#       findFreqTerms(TDM_rubio, 10)[1:5]), nrow = 6, byrow = TRUE, dimnames = list(row_names ,col_names ) )
# 
# word_df<-as.data.frame(word_mat)
# 
# 
# 
# print(xtable(word_df), type='html', comment=FALSE, include.rownames=TRUE, 
#       html.table.attributes='border="3" align="center" ' )


```




```{r, echo=FALSE}

#findAssocs_candidate(TDM_trump,"country")
# findAssocs_candidate(TDM_sanders,"people")
# findAssocs_candidate(TDM_clinton,"people")
# findAssocs_candidate(TDM_fiorina,"people")

```

Here are the five most frequent words used by the candidates in tabular form. 


```{r, echo=FALSE}
## Build a matrix owith counts of specific words for a list of candidates

## First convert TDMs to Data Frames

a<-as.matrix(TDM_trump)
b<-as.data.frame(a)
df_trump<-b
colnames(df_trump)<-"trump"

words_trump<-sum(df_trump)
vocab_trump<-nrow(df_trump)

a<-as.matrix(TDM_sanders)
b<-as.data.frame(a)
df_sanders<-b
colnames(df_sanders)<-"sanders"

words_sanders<-sum(df_sanders)
vocab_sanders<-nrow(df_sanders)

a<-as.matrix(TDM_fiorina)
b<-as.data.frame(a)
df_fiorina<-b
colnames(df_fiorina)<-"fiorina"

words_fiorina<-sum(df_fiorina)
vocab_fiorina<-nrow(df_fiorina)

a<-as.matrix(TDM_clinton)
b<-as.data.frame(a)
df_clinton<-b
colnames(df_clinton)<-"clinton"

words_clinton<-sum(df_clinton)
vocab_clinton<-nrow(df_clinton)
## merge the data frames

merged_candidates<-merge(df_trump, df_sanders, by=0, all=TRUE)
rownames(merged_candidates) <- merged_candidates$Row.names
merged_candidates$Row.names <- NULL
merged_candidates<-merge(merged_candidates, df_clinton, by="row.names", all=TRUE)
rownames(merged_candidates)<-merged_candidates$Row.names
merged_candidates$Row.names <- NULL
merged_candidates<-merge(merged_candidates, df_fiorina, by="row.names", all=TRUE)


## fix NAs
merged_candidates[is.na(merged_candidates)]<-0

##Get rid of self references
merged_candidates["clinton", "clinton"]<-0
merged_candidates["trump", "trump"]<-0
merged_candidates["sanders", "sanders"]<-0
merged_candidates["fiorina", "fiorina"]<-0

## sum all
merged_candidates$all <- merged_candidates$trump + merged_candidates$clinton + merged_candidates$sanders + merged_candidates$fiorina

## sort it
merged_candidates<-merged_candidates[with(merged_candidates, order(-all)), ]

#merged_candidates <- merged_candidates[merged_candidates$all>50,]
## convert Row.names to a factor
merged_candidates$Row.names<-as.factor(merged_candidates$Row.names)

merged_candidates<-merged_candidates[complete.cases(merged_candidates),]

## This is what the data look like
#      Row.names trump sanders clinton fiorina all
# 1810    people    33      85      53      10 181
# 2483     think     9      55      90       9 163
# 1050     going    44      44      45      10 143
# 561    country    34      70      25       1 130
# 1363      know    23      26      56      19 124
# 2704      well     9      31      56       8 104

```

```{r, echo=FALSE, results="asis"}

library(xtable)

c_1<-merged_candidates[with(merged_candidates, order(-clinton)), ]
s_1<-merged_candidates[with(merged_candidates, order(-sanders)), ]
f_1<-merged_candidates[with(merged_candidates, order(-fiorina)), ]
t_1<-merged_candidates[with(merged_candidates, order(-trump)), ]

compare_mf <- rbind(c_1[1:5,], f_1[1:5,], s_1[1:5,], t_1[1:5,])
colnames(compare_mf)<-c("word", "trump", "sanders", "clinton", "fiorina", "sum")

## compute column sums for each candidate
trump_sum<-sum(merged_candidates[,2])
sanders_sum<-sum(merged_candidates[,3])
clinton_sum<-sum(merged_candidates[,4])
fiorina_sum<-sum(merged_candidates[,5])
sum_sum<-sum(merged_candidates[,6])
sum_row<-c("SUM", trump_sum, sanders_sum, clinton_sum, fiorina_sum, sum_sum)

#bind rows
compare_mf$word<-as.character(compare_mf$word)
compare_mf<-rbind(compare_mf, sum_row)

print(xtable(unique(compare_mf), digits=0), type='html', comment=FALSE, include.rownames=FALSE, 
      html.table.attributes='border="3" align="center" ' )

```

  
Word counts differ widely. For instance Carly Fiorina said _"government"_" a total of forty times in her two debates, while Donald Trump didn't say it at all.  
The total number of words spoken by Carly Fiorina was `r words_fiorina` and her vocabularly of distinct words was `r vocab_fiorina`. By comparison, Bernie Sanders said `r words_sanders` total words, with a vocabulary of `r vocab_sanders` words.  


####NORMALIZED WORD FREQUENCIES

From the above, there may be information in comparing words used frequently by one candidate to frequency of use by another. Here is a graph of the "top" words used by all candidates. From the above we need to be careful to normalize the word count, 
$\nu_{i} = W_{i} / \sum_{k=1}^{N} W_{k}$, where $\nu_{i}$ is the normalized frequency of word $i$ with count $W_{i}$.


In the graph below the $\nu_{i}$ for each candidate are plotted for the most-used words as measured for the ensemble of all candidates. 

```{r, echo=FALSE, fig.align="center", fig.height= 5, fig.width=8, message=FALSE, warning=FALSE}

library(reshape2)

merged_plot<-melt(merged_candidates[merged_candidates$all>50,])
colnames(merged_plot)<-c("words", "candidate", "count")

library(ggplot2)

# p <- ggplot(merged_plot, aes(x = words, y = count, color=candidate))
# p <- p + geom_point(size=3, pch=19)
# p <- p + theme(axis.text.x=element_text(face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"))
# p <- p + coord_flip()
# p <- p + ggtitle("CANDIDATE WORD COUNT")
# p <- p + xlab("word")
# p <- p + ylab("count")
# p 

```
```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=8, message=FALSE, warning=FALSE}
mc<-merged_candidates
mc$rank <- 1:nrow(mc)
##normalize
mc$clinton<-mc$clinton/sum(mc$clinton)
mc$sanders<-mc$sanders/sum(mc$sanders)
mc$trump<-mc$trump/sum(mc$trump)
mc$fiorina<-mc$fiorina/sum(mc$fiorina)

mc_plot<-mc[mc$all>30,]

p <- ggplot(mc_plot, aes(x = rank, size=3))
#p <- p + geom_line(size=0.5)
p <- p + geom_point(aes(y = clinton, color="clinton"), size=3)
p <- p + geom_point(aes(y = sanders, color="sanders"), size=3)
p <- p + geom_point(aes(y = trump, color="trump"), size=3)
p <- p + geom_point(aes(y = fiorina, color="fiorina"), size=3)
p <- p + scale_colour_manual(values = c("blue", "red","darkblue", "darkred"))

p <- p + theme(axis.text.x=element_text(angle=90, hjust=1, vjust = 0.5, face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"),legend.title=element_blank())
#p <- p + coord_flip()
p <- p + scale_x_discrete(breaks=1:length(mc$Row.names),
        labels=mc$Row.names)
p <- p + ggtitle("MOST-USED WORD FREQUENCIES")
p <- p + xlab("")
p <- p + ylab("normalized frequency")
p 

```

This is much more informative. For instance, Carly Fiorina mentions the word _"government"_  more than two percent of her word usage, whereas Donald Trump doesn't mention the word at all. Notice that both Bernie Sanders and Donald Trump mention the word _"wall"_ significantly more than their competitors, while Bernie Sanders alone mentions the word _"street"_ with comparably high frequency. We'll revisit this below.   
Many of the most frequent words convey little information about candidate positions. As with the wordcloud analysis, they convey mostly style. 

####COEFFICEINT OF VARIATION

To highlight differences between candidates we can look at the standard deviation of the word frequencies normalized to the mean value, or the Coefficient of Variation. 

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=8, message=FALSE, warning=FALSE}

##calculate mean

mc_plot$mean <- rowSums(mc_plot[,c(2,3,4,5)])/4

## subtract
mc_plot$sanders<-(mc_plot$sanders - mc_plot$mean)
mc_plot$clinton<-(mc_plot$clinton - mc_plot$mean)
mc_plot$fiorina<-(mc_plot$fiorina - mc_plot$mean)
mc_plot$trump<-(mc_plot$trump - mc_plot$mean)

## calc squared dev
mc_plot$sdev<- sqrt( ((mc_plot$sanders)^2 + (mc_plot$clinton)^2 + (mc_plot$fiorina)^2 + (mc_plot$trump)^2)/(4-1))

mc_plot$sanders<-(mc_plot$sanders/mc_plot$sdev)
mc_plot$clinton<-(mc_plot$clinton/mc_plot$sdev)
mc_plot$fiorina<-(mc_plot$fiorina/mc_plot$sdev)
mc_plot$trump<-(mc_plot$trump/mc_plot$sdev)



p <- ggplot(mc_plot, aes(x = rank, y = sdev/mean))
p <- p + geom_line(size=1.5, color="#A25DDFCC")


p <- p + theme(axis.text.x=element_text(angle=90, hjust=1, vjust=.5, face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"),legend.title=element_blank())
#p <- p + coord_flip()
p <- p + scale_x_discrete(breaks=1:length(mc$Row.names),
        labels=mc$Row.names)
p <- p + ggtitle("VARIATION OF FREQUENT WORDS")
p <- p + xlab("")
p <- p + ylab("coefficient")
p 

```

Words with the highest coefficient of variation $c_v = \sigma/\mu$, where $\sigma$ is the standard deviation and $\mu$ is the mean value, are apparent. These include _"government"_, _"street"_ and others identified above.

###KEYWORDS
A way to address the problem of "filler" words is to select for specific _"key words"_ relevant to the topics of interest. The list below combines some "hand selected" and well as those with high coefficeint of variation. 

```{r, echo = 6:7, fig.align="center", fig.height=4, fig.width=8, message=FALSE, warning=FALSE }

library(reshape2)

merged_plot<-melt(merged_candidates)
colnames(merged_plot)<-c("words", "candidate", "count")

key_words = c("tax", "government", "climate", "class", "wall", "street","terror", "economy", "immigrant", "america", "veteran", "drug", "health", "gun", "education", "bankruptcy", "money", "women", "war", "rights", "abortion", "violence", "theyre", "going" )

##select just key_words
merged_plot<-merged_plot[merged_plot$word %in% key_words, ]
##eliminate a special case
merged_plot<-merged_plot[!merged_plot$candidate=="all",]




mc<-mc[mc$Row.names %in% key_words,]
mc$rank<-1:nrow(mc)


p <- ggplot(mc, aes(x = rank, size=3))
#p <- p + geom_line(size=0.5)
p <- p + geom_point(aes(y = clinton, color="clinton"), size=3)
p <- p + geom_point(aes(y = sanders, color="sanders"), size=3)
p <- p + geom_point(aes(y = trump, color="trump"), size=3)
p <- p + geom_point(aes(y = fiorina, color="fiorina"), size=3)
p <- p + theme(axis.text.x=element_text(angle=90, hjust=1,face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"),legend.title=element_blank())
p <- p + scale_colour_manual(values = c("blue", "red","darkblue", "darkred"))
#p <- p + coord_flip()
p <- p + scale_x_discrete(breaks=1:25,
        labels=mc$Row.names)
p <- p + ggtitle("SELECTED KEYWORD FREQUENCIES")
p <- p + xlab("")
p <- p + ylab("normalized frequency")
p 

```

An apparent problem is that many of the words of interest have fairly low frequencies. To better distinguish signficant differences, we can calculate a simple $z$ statistic by taking the mean and standard deviation of the word frequencies.

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=8, message=FALSE, warning=FALSE}

##calculate mean

mc_plot<-mc

mc_plot$mean <- rowSums(mc_plot[,c(2,3,4,5)])/4

## subtract
mc_plot$sanders<-(mc_plot$sanders - mc_plot$mean)
mc_plot$clinton<-(mc_plot$clinton - mc_plot$mean)
mc_plot$fiorina<-(mc_plot$fiorina - mc_plot$mean)
mc_plot$trump<-(mc_plot$trump - mc_plot$mean)

## calc squared dev
mc_plot$sdev<- sqrt( ((mc_plot$sanders)^2 + (mc_plot$clinton)^2 + (mc_plot$fiorina)^2 + (mc_plot$trump)^2)/(4-1))

mc_plot$sanders<-(mc_plot$sanders/mc_plot$sdev)
mc_plot$clinton<-(mc_plot$clinton/mc_plot$sdev)
mc_plot$fiorina<-(mc_plot$fiorina/mc_plot$sdev)
mc_plot$trump<-(mc_plot$trump/mc_plot$sdev)



p <- ggplot(mc_plot, aes(x = rank, size=3))
#p <- p + geom_line(size=0.5)
p <- p + geom_point(aes(y = clinton, color="clinton"), size=3)
p <- p + geom_point(aes(y = sanders, color="sanders"), size=3)
p <- p + geom_point(aes(y = trump, color="trump"), size=3)
p <- p + geom_point(aes(y = fiorina, color="fiorina"), size=3)
p <- p + theme(axis.text.x=element_text(angle=90, hjust=1, face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"),legend.title=element_blank())
p <- p + scale_colour_manual(values = c("blue", "red","darkblue", "darkred"))
#p <- p + coord_flip()
p <- p + scale_x_discrete(breaks=1:length(mc$Row.names),
        labels=mc$Row.names)
p <- p + ggtitle("Z_STATISTICS MOST FREQUENT WORDS")
p <- p + xlab("")
p <- p + ylab("normalized frequency deviation")
p 

```

This approach highlights some fairly interesting differences. For instance:  
- Carly Fiorina's use of the word _"government"_ differs by almost two standard deviations from the other candidates.  
- _"tax"_ is used significantly more by Republicans than Democrats as is the word _"money"_.  
- Bernie Sanders is the top user of issue words like _"health"_, _"gun"_, _"economy"_, and _"veteran"_ and many others.  
- _"women"_ are mentioned by all candidates except Donald Trump.  
- _"wall"_ is mentioend significantly more by Donald Trump and Bernie Sanders than by Hilary Clinton or Carly Fiorina.  

###WORD ASSOCIATIONS FROM N-GRAM TOKENIZATION

Since word fequency alone does not convey context, let's look at word associations to see if we can clarify intent and context.  
To do this, let's start with bigram tokenization of the text associated with some of the issue key words. Using the {RWeka} package we can create tables of bi- and tri-grams, which can then be searched using standard regualr expressions.

```{r eval=FALSE}
bigram_table[grep(word, rownames(bigram_table), ignore.case=TRUE)]
```



```{r, echo=FALSE}

library(RWeka)

## create Candidate Bigrams

bigramTokenizer <- function(x) NGramTokenizer(x, 
                                Weka_control(min = 2, max = 2))

trump_bigrams <- sort(table(bigramTokenizer(trump_all)), decreasing = TRUE)
sanders_bigrams <- sort(table(bigramTokenizer(sanders_all)), decreasing = TRUE)
clinton_bigrams <- sort(table(bigramTokenizer(clinton_all)), decreasing = TRUE)
fiorina_bigrams <- sort(table(bigramTokenizer(fiorina_all)), decreasing = TRUE)

## find candidate trigrams

trigramTokenizer <- function(x) NGramTokenizer(x, 
                                Weka_control(min = 3, max = 3))

trump_trigrams <- sort(table(trigramTokenizer(trump_all)), decreasing = TRUE)
sanders_trigrams <- sort(table(trigramTokenizer(sanders_all)), decreasing = TRUE)
clinton_trigrams <- sort(table(trigramTokenizer(clinton_all)), decreasing = TRUE)
fiorina_trigrams <- sort(table(trigramTokenizer(fiorina_all)), decreasing = TRUE)

trigram_finder <- function(word, trigram_table){
    relevant_trigrams <- trigram_table[grep(word, rownames(trigram_table), ignore.case=TRUE)]
    return(relevant_trigrams)
}

bigram_finder <- function(word, bigram_table){
    relevant_bigrams <- bigram_table[grep(word, rownames(bigram_table), ignore.case=TRUE)]
    return(relevant_bigrams)
}

```


####"TAX" IN CONTEXT

The word _"tax"_ is heavily used by Carly Fiorina and Donald Trump. Let's look at the bigrams starting with the word _"tax"_

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=6, warning=FALSE, message=FALSE}


word_sample <- "^tax"
a<-bigram_finder(word_sample, fiorina_bigrams)
a<-as.data.frame(a)
colnames(a)<-word_sample

bigram_freq<-a

library(ggplot2)

p <- ggplot(bigram_freq, aes(x=rownames(bigram_freq), y = bigram_freq[,1]))
p <- p + geom_bar(fill="#751B1C", stat="identity")
p <- p + ggtitle(paste0("FIORINA bigram sample: \"", word_sample,"\" "))
p <- p + xlab("bigrams")
p <- p + ylab("count")
p <- p + theme(axis.text.x=element_text(face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"))
p <- p + scale_y_continuous(limits=c(0, 10), breaks=c(0, 2, 4, 6, 8, 10))
p <- p + coord_flip()
plot_tax_fiorina <- p
p

```

We can drill down a bit on _"tax code"_ using trigrams. While the number of cases are limited, we get a slight hint of her position here. 

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=6, warning=FALSE, message=FALSE}


word_sample <- "^tax code"
a<-bigram_finder(word_sample, fiorina_trigrams)
a<-as.data.frame(a)
colnames(a)<-word_sample

bigram_freq<-a

library(ggplot2)

p <- ggplot(bigram_freq, aes(x=rownames(bigram_freq), y = bigram_freq[,1]))
p <- p + geom_bar(fill="#751B1C", stat="identity")
p <- p + ggtitle(paste0("FIORINA trigram sample: \"", word_sample,"\" "))
p <- p + xlab("bigrams")
p <- p + ylab("count")
p <- p + theme(axis.text.x=element_text(face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"))
p <- p + scale_y_continuous(limits=c(0, 10), breaks=c(0, 2, 4, 6, 8, 10))
p <- p + coord_flip()
plot_tax_fiorina <- p
p

```


 

Donald Trump's choice of words paring with _"tax"_ are mostly negative.

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=6, warning=FALSE, message=FALSE}


word_sample <- "^tax"
a<-bigram_finder(word_sample, trump_bigrams)
a<-as.data.frame(a)
colnames(a)<-word_sample

bigram_freq<-a

library(ggplot2)

p <- ggplot(bigram_freq, aes(x=rownames(bigram_freq), y = bigram_freq[,1]))
p <- p + geom_bar(fill="#751B1C", stat="identity")
p <- p + ggtitle(paste0("TRUMP bigram sample: \"", word_sample,"\" "))
p <- p + xlab("bigrams")
p <- p + ylab("count")
p <- p + theme(axis.text.x=element_text(face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"))
p <- p + scale_y_continuous(limits=c(0, 10), breaks=c(0,2,4,6,8,10))
p <- p + coord_flip()
p

```

####"WALL" IN CONTEXT

The word _"wall"_ is used frequently by both Bernie Sanders and Donald Trump. We can clarify the context by looking at bigrams. In this case it's clear Bernie Sanders is referring exclusively to "wall street" while Donald Trump mostly refers to his proposal to build border walls. 


```{r, echo=FALSE, fig.align="center", fig.height=2, fig.width=6, warning=FALSE, message=FALSE}


word_sample <- "^wall"
a<-bigram_finder(word_sample, sanders_bigrams)
a<-as.data.frame(a)
colnames(a)<-word_sample

bigram_freq<-a

#print(bigram_freq)

library(ggplot2)

p <- ggplot(bigram_freq, aes(x=rownames(bigram_freq), y = bigram_freq[,1]))
p <- p + geom_bar(fill="#415DA3", stat="identity")
p <- p + ggtitle(paste0("SANDERS bigram sample: \"", word_sample,"\" "))
p <- p + xlab("bigrams")
p <- p + ylab("count")
p <- p + theme(axis.text.x=element_text(face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"))
p <- p + scale_y_continuous(limits=c(0, 31), breaks=c(0,10,20,30))
p <- p + coord_flip()
plot_tax_sanders <- p

plot_tax_sanders

```

```{r, echo=FALSE, fig.align="center", fig.height=3, fig.width=6, warning=FALSE, message=FALSE}


word_sample <- "^wall"
a<-bigram_finder(word_sample, trump_bigrams)
a<-as.data.frame(a)
colnames(a)<-word_sample

bigram_freq<-a

library(ggplot2)

p <- ggplot(bigram_freq, aes(x=rownames(bigram_freq), y = bigram_freq[,1]))
p <- p + geom_bar(fill="#751B1C", stat="identity")
p <- p + ggtitle(paste0("TRUMP bigram sample: \"", word_sample,"\" "))
p <- p + xlab("bigrams")
p <- p + ylab("count")
p <- p + theme(axis.text.x=element_text(face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"))
p <- p + scale_y_continuous(limits=c(0, 10), breaks=c(0,2,4,6,8,10))
p <- p + coord_flip()
p

```


####"THEYRE" IN CONTEXT

Donald Trump uses the word _"theyre"_ signficantly more than other candidates. The context as revealed by bigrams almost sounds like the script of a zombie movie. "theyre going", "theyre south", "theyre feeding", theyre coming". While no position is advocated here, this does begin to hint at a sentiment that, whoever "they" are, they're after us. 

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=6, warning=FALSE, message=FALSE}


word_sample <- "^theyre"
a<-bigram_finder(word_sample, trump_bigrams)
a<-as.data.frame(a)
colnames(a)<-word_sample

bigram_freq<-a

library(ggplot2)

p <- ggplot(bigram_freq, aes(x=rownames(bigram_freq), y = bigram_freq[,1]))
p <- p + geom_bar(fill="#751B1C", stat="identity")
p <- p + ggtitle(paste0("TRUMP bigram sample: \"", word_sample,"\" "))
p <- p + xlab("bigrams")
p <- p + ylab("count")
p <- p + theme(axis.text.x=element_text(face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"))
p <- p + scale_y_continuous(limits=c(0, 10), breaks=c(0,2,4,6,8,10))
p <- p + coord_flip()
p

```


_NOTE:_ after this anlaysis was completed, the [New York Times](http://www.nytimes.com/interactive/2015/12/05/us/politics/donald-trump-talk.html) published a story on linguistic style of Donald Trump. Conclusions are similar. 



###CONCLUSIONS
Word-clouds provide insight into differences in style but do not delineate well between candiddate positions. Surprisingly opposing candidates can have very similar word clouds.
Looking at "most frequent" provides limited insight into differences between candidate positions, though many frequently used words provide no insight 
By looking at the co-efficient of variance and selecting for key words, we can highlight striking differences between candidates. 
Bigrams provide key context difference and being to hint at sentiment. 

###NEXT STEPS

My next step is to expand the text volume by adding more debate text. Since the data suggest candidate speech is largely consistent debate to debate, it might also be beneificial to include speech transscripts if these can be found easily online.  
Another avenue is to use pre-defined word vectors to coax simiilarities from the texts. This might help narrow the 