---
title: "DEBATE WORDS"
author: "Winston Saunders"
date: "November 18, 2015"
output: 
    html_document:
        theme: united
---

###SUMMARY
This analysis is based on the premise that widening political divisions are not just about "what to do" but, more fundamentally, about perceptions of "what the priorities are."  
Perceptions of priorities are influenced, in part, by how frequently we hear about an issue. In turn, those who speak about issues we care about tend to attract out attention. It's the nature of this positive feedback loop that is the subject of exploration here; how can we detect, using statistics, systematic differences in language reflecting priorities and biases.  
Here, using standard NLP (Natural Lanuguage Processing) techniques, I explore this question looking for differences in the texts from recent Republican and Democratic presidential debates. 
Key findings are:  
1. "Wordcloud" visualization reveal differences between candidates, though similarities are just as surprising.  
2. Frequency analysis of keywords highlights strong differences between candidatates, but misses important context.  
3. Bigram toeknization and word-stem searches begin to reveal subtleties of meaning.   


###DATA SOURCES AND METHODS
The text of the presidential debates are downloaded from the [UCSB Presidency Project](http://www.presidency.ucsb.edu/debates.php). Transcripts were pasted into Apple Pages and stored as unformatted .txt files. From that point all processing is done with __R__ using capabilities of {tm} and associated libraries.  

  

###CANDIDATE WORD-CLOUDS

The quickest and most visual method to compare texts is word-frequency analysis using wordclouds. 
Not surprisingly, word choices vary between candidates. However, there are also some striking similarities.  

```{r, echo=FALSE, warning=FALSE, message=FALSE}


load_debate_text <- function(file_name){

## GET THE DATA
    
    ## data are the raw text files downloaded from the UCSB website.
    
    directory <- "/Users/winstonsaunders/Documents/Presidential_Debates_2015/"
    mydata <- read.table(paste0(directory, file_name), header=FALSE, sep="\n", stringsAsFactors = FALSE, quote = "")

    ## This is what a sample of the the raw data looks like
    ## mydata[100:105,1]
        # [1] "QUINTANILLA: Hold it. We'll cut it back to..."
        # [2] "QUICK: Dr. Carson, let's talk about taxes."
        # [3] "You have a flat tax plan of 10 percent flat taxes..."
        # [4] "If you were to took a 10 percent tax..."
        # [5] "So what analysis got you to the point where you think this will work?"
        # [6] "CARSON: Well, first of all..."

## ADD COLUMNS OF NUMBERS AND SEPARATE TEXT FROM CANDIDATE NAMES
    ## Add number column
    #mydata$n<-1:nrow(mydata)
    
    ## get rid of "..."
    #mydata$V1<-gsub("...", "", mydata$V1)
    
    ## This regex gets rid of all but the capitalized names
    mydata$name <- gsub(":.+|^[A-Z][^A-Z].+", "", mydata$V1 )

    ## Fill in the blank rows
    for (i in 2: nrow(mydata)){
        if (mydata$name[i] == "") mydata$name[i] <- mydata$name[i-1]
    }

    ## CREATE COLUMN OF DEBATE TEXT AND CLEAN UP BLANK SPACES
    mydata$text <- gsub (".", "", mydata$V1, fixed=TRUE)
    mydata$text <- gsub ("'", "", mydata$text)
    mydata$text <- gsub ("=", "", mydata$text)
    mydata$text <- gsub ("[A-Z]{2,}: ", "", mydata$text)
    mydata$text <- tolower(mydata$text)
    mydata$text <- gsub ("   ", " ", mydata$text)
    mydata$text <- gsub ("  ", " ", mydata$text)
    ##stem a few words
    mydata$text <- gsub ("taxes", "tax", mydata$text)
    mydata$text <- gsub ("guns", "gun", mydata$text)
    mydata$text <- gsub ("veterans", "veteran", mydata$text)
    mydata$text <- gsub ("terrorists", "terror", mydata$text)
    mydata$text <- gsub ("terrorism", "terror", mydata$text)
    
    #mydata$text <- gsub ("americans", "american", mydata$text)
    #mydata$text <- gsub ("american", "america", mydata$text)

    
    
    
    ## the data frame now contains four columns which look like this...
    
        #   50 Our greatest days lie ahead...
        #   51 QUINTANILLA: Mr. Trump?
        #   52 TRUMP: I think maybe ...[laughter]
        #     n        name
        # 50 50       RUBIO
        # 51 51 QUINTANILLA
        # 52 52       TRUMP
        # text
        # 50  Our greatest days lie ahead ....
        # 51  Mr. Trump?
        # 52  I think maybe my greatest weakness ... [laughter]
    
    return(mydata)
}

```

```{r echo=FALSE, warning=FALSE, message=FALSE}

candidate_text <- function(candidate, mydata){
    ## 
    ## GET CANDIDATE TEXT
    ##
    ## Assumes load_debate_text has been run and the debate text is stored in "mydata"
    ## creates a text list
    
  
    
    ## filter for candidate
    text<-mydata$text[mydata$name==candidate]
    #text<-paste(text, collapse = " ")

    return(text)


}
```


```{r echo=13:22, warning=FALSE, message=FALSE}

    library(tm)
    library(RWeka)

text_tc <- function(mydata){
    ## 
    ## 
    ##
    ## Assumes text is stored in "mydata"
    ## creates a Corpus from the text
    ## filters some words out 

    require(tm)
    require(SnowballC)

    t_c <- Corpus(VectorSource(text))
    t_c <- tm_map(t_c, content_transformer(tolower))
    t_c <- tm_map(t_c, removePunctuation)
    t_c <- tm_map(t_c, removeNumbers)
    t_c <- tm_map(t_c, removeWords, stopwords("english"))
    t_c <- tm_map(t_c, removeWords, c("applause", "thats", "laughter", "dont", "back", "can", "get", "cant", "come", "big", "inaudible"))

    return(t_c)   
    
} 

```


```{r echo=FALSE, warning=FALSE, message=FALSE}

    library(tm)
    library(RWeka)
    library(SnowballC)

candidate_text_tc <- function(candidate, mydata){
    ## 
    ## GET CANDIDATE DATA
    ##
    ## Assumes load_debate_text has been run and the debate text is stored in "mydata"
    ## creates a Corpus from the candidate text
    
  
    
    ## filter for candidate
    text<-mydata$text[mydata$name==candidate]
    text<-paste(text, collapse = " ")

    require(tm)
    require(SnowballC)

    t_c <- Corpus(VectorSource(text))
    t_c <- tm_map(t_c, content_transformer(tolower))
    t_c <- tm_map(t_c, removePunctuation)
    t_c <- tm_map(t_c, removeNumbers)
    t_c <- tm_map(t_c, removeWords, stopwords("english"))
    t_c <- tm_map(t_c, removeWords, c("applause", "thats", "laughter", "dont", "back", "can", "get", "cant", "come", "big", "inaudible"))

    #t_c <- tm_map(t_c, stemDocument)
    
    return(t_c)   
    
} 

```

```{r echo=FALSE, warning=FALSE, message=FALSE}

library(wordcloud)
library(RColorBrewer)

 color_map2<-c("#00003B","#041851","#4E0812" ,"#051E65", "#650A16", "#103374",  "#9F1A2D", "#482B25")

c_wordcloud <- function(t_c){
    
    ## 
    ## CREATE WORD CLOUD
    ##
    ## Assumes a text Corpus has been created
    ## 
    
    set.seed(8675309)
    
   
    color_map2<-c("#00003B","#041851","#4E0812" ,"#051E65", "#650A16", "#103374",  "#9F1A2D", "#482B25")
    
    wordcloud(t_c, scale=c(4,0.4), max.words=200, min.freq=5, random.order=FALSE, rot.per=0.2, use.r.layout=FALSE, colors=color_map2)

    

}

```



```{r, echo=FALSE, warning=FALSE, message=FALSE}

    ## GET ALL REPUB DEBATES
     file_name<-"Republican Candidates Debate in Boulder Colorado October 28 2015.txt"
     r_oct<-load_debate_text(file_name)
     #r_oct$date<-"Oct"
     file_name<-"Republican Candidates Debate in Milwaukee Wisconsin November 10 2015.txt"
     r_nov<-load_debate_text(file_name)
     #r_nov$date<-"Nov"
    
    r_all = rbind(r_oct, r_nov)
        ## slim down data a bit
        ## reduce data to about 30%
#         r_all<-r_all[grepl("RUBIO", r_all$name)|
#                          grepl("CARSON", r_all$name)|
#                          grepl("TRUMP", r_all$name)|
#                          grepl("CRUZ", r_all$name)|
#                          grepl("HUCKABEE", r_all$name)|
#                          grepl("FIORINA", r_all$name), ]
    
    
    
    ## GET ALL DEMO DEBATES
     file_name<-"Democratic Candidates Debate in Las Vegas Nevada October 13 2015 .txt"
     d_oct<-load_debate_text(file_name)
     file_name<-"Democratic Candidates Debate in Des Monies Iowa November 14 2015 .txt"
     d_nov<-load_debate_text(file_name)
    
    d_all = rbind(d_oct, d_nov)
    
    ## CREATE TCs FOR EACH CANDIDATE
    trump_all<-candidate_text_tc("TRUMP",r_all)
    rubio_all<-candidate_text_tc("RUBIO",r_all)
    fiorina_all<-candidate_text_tc("FIORINA",r_all)
    carson_all<-candidate_text_tc("CARSON",r_all)
    cruz_all<-candidate_text_tc("CRUZ",r_all)
    huckabee_all<-candidate_text_tc("HUCKABEE",r_all)
    bush_all<-candidate_text_tc("BUSH",r_all)

    clinton_all<-candidate_text_tc("CLINTON",d_all)
    sanders_all<-candidate_text_tc("SANDERS",d_all)
    
    
    
```






<style>
  .col2 {
    columns: 2 300px;         /* number of columns and width in pixels*/
    -webkit-columns: 2 300px; /* chrome, safari */
    -moz-columns: 2 300px;    /* firefox */
  }
  .col3 {
    columns: 3 200px;
    -webkit-columns: 3 200px;
    -moz-columns: 3 200px;
  }
</style>




Let's first compare the word clouds of candidates using the {wordcloud} package.   

####TRUMP V. SANDERS

Differences between Donald Trump's and Bernie Sanders's dialogue at the debates are evident. Bernie's word cloud is larger due to have spoken more wtotal words. However, despite the differences, what is most surprising is the similarity of the clouds; word choices like _people_, _country_, and _going_ are common to both. Despite strong differences in policy, these word clouds reveal little about them.

<div class="col2">


```{r, echo=TRUE, message=FALSE, fig.align="center", results="asis"}
c_wordcloud(trump_all)
c_wordcloud(sanders_all)
```

</div>

####HILARY V. CARLY

In this case word clouds couldn't be more different. Hilary's emphasizes _think_ and _people_ while Carly's, a former business woman, primarily emphasizes _government_. However, there is no context to judge, for isntance, what Ms. Fiorina's opinions or sentiments toward government are. Note again Hilary's wordcloud is larger than Ms. Fiorina's.

<div class="col2">


```{r, echo=TRUE, message=FALSE, fig.align="center", results="asis"}
c_wordcloud(clinton_all)
c_wordcloud(fiorina_all)
```

</div>

####CRUZ V. HUCKABEE

Ted Cruz's wordcloud seems to emphasize wonkish financial technicalities, like _taxes_ and _washington_, while that of Mike Huckabee, a former minister, mixes language of Mr. Trump and Ms. Fiorina together. Again, no sentiment can be extracted.

<div class="col2">


```{r, echo=TRUE, message=FALSE, fig.align="center", results="asis"}
c_wordcloud(cruz_all)
c_wordcloud(huckabee_all)
```

</div>


###STAYING ON MESSAGE: COMPARING DEBATES

We can also split the text by debate. Since the debates cover different topics and questions, one might expect to see this reflected in the text of the separate dialogues. What's surprising here is how comparable the language of each candidate is between the debates. Perhaps the candidates are more interested in staying on message than answering questions directly?

<div class="col2">


```{r, echo=TRUE, message=FALSE, fig.align="center", results="asis"}
c_wordcloud(candidate_text_tc("TRUMP", r_oct))
c_wordcloud(candidate_text_tc("TRUMP", r_nov))
```
</div>

<div class="col2">


```{r, echo=TRUE, message=FALSE, fig.align="center", results="asis"}
c_wordcloud(candidate_text_tc("SANDERS", d_oct))
c_wordcloud(candidate_text_tc("SANDERS", d_nov))
```
</div>


###WORD FREQUENCY

We can check word frequency directly by simply tokenizing the text and counting single words. In a sense this is equivalnt to the wordcloud analysia, but it more quantitative. To do this analysis some additional words like "thats", "dont", "back", "can", "get", "cant", and "come" are suppressed.  


<style>
tr:hover {background-color: #f5f5f5}
table { 
    width: 80%;
    display: table;
    border-collapse: collapse;
    border-spacing: 18px;
    border-color: #AAAAFF;
    background-color: #D5DAB6;
    padding: 2px;
    font: 16px arial, sans-serif;
}
th, td{
    text-align: center;
}
</style>


```{r, echo=FALSE}

## Create Term_Document_Matrices

TDM_trump <- TermDocumentMatrix(trump_all)
TDM_rubio <- TermDocumentMatrix(rubio_all)
TDM_fiorina <- TermDocumentMatrix(fiorina_all)
TDM_carson <- TermDocumentMatrix(carson_all)
TDM_cruz <- TermDocumentMatrix(cruz_all)
TDM_huckabee <- TermDocumentMatrix(huckabee_all)
TDM_bush <- TermDocumentMatrix(bush_all)

TDM_clinton <- TermDocumentMatrix(clinton_all)
TDM_sanders <- TermDocumentMatrix(sanders_all)

```



```{r, echo=FALSE, results='asis', fig.align='center'}

library(xtable)

## Create table of frequent terms
## note: The Frequent Term utility does not rank by order. It produces a character vector of terms in x which occur more or equal often than lowfreq times and less or equal often than highfreq times.

# row_names <- c("Trump", "Sanders", "Clinton", "Fiorina", "Cruz", "Rubio" )
# col_names <- c("most frequent", "Second", "Third", "Fourth", "Fifth")

# word_mat<-matrix(
#     c(findFreqTerms(TDM_trump, 30)[1:5],
#       findFreqTerms(TDM_sanders, 30)[1:5],
#       findFreqTerms(TDM_clinton, 30)[1:5],
#       findFreqTerms(TDM_fiorina, 10)[1:5],
#       findFreqTerms(TDM_cruz, 10)[1:5],
#       findFreqTerms(TDM_rubio, 10)[1:5]), nrow = 6, byrow = TRUE, dimnames = list(row_names ,col_names ) )
# 
# word_df<-as.data.frame(word_mat)
# 
# 
# 
# print(xtable(word_df), type='html', comment=FALSE, include.rownames=TRUE, 
#       html.table.attributes='border="3" align="center" ' )


```




```{r, echo=FALSE}

#findAssocs_candidate(TDM_trump,"country")
# findAssocs_candidate(TDM_sanders,"people")
# findAssocs_candidate(TDM_clinton,"people")
# findAssocs_candidate(TDM_fiorina,"people")

```

This table of the most frequent word used by each candidate. 


```{r, echo=FALSE}
## Build a matrix owith counts of specific words for a list of candidates

## First convert TDMs to Data Frames

a<-as.matrix(TDM_trump)
b<-as.data.frame(a)
df_trump<-b
colnames(df_trump)<-"trump"

words_trump<-sum(df_trump)
vocab_trump<-nrow(df_trump)

a<-as.matrix(TDM_sanders)
b<-as.data.frame(a)
df_sanders<-b
colnames(df_sanders)<-"sanders"

words_sanders<-sum(df_sanders)
vocab_sanders<-nrow(df_sanders)

a<-as.matrix(TDM_fiorina)
b<-as.data.frame(a)
df_fiorina<-b
colnames(df_fiorina)<-"fiorina"

words_fiorina<-sum(df_fiorina)
vocab_fiorina<-nrow(df_fiorina)

a<-as.matrix(TDM_clinton)
b<-as.data.frame(a)
df_clinton<-b
colnames(df_clinton)<-"clinton"

words_clinton<-sum(df_clinton)
vocab_clinton<-nrow(df_clinton)
## merge the data frames

merged_candidates<-merge(df_trump, df_sanders, by=0, all=TRUE)
rownames(merged_candidates) <- merged_candidates$Row.names
merged_candidates$Row.names <- NULL
merged_candidates<-merge(merged_candidates, df_clinton, by="row.names", all=TRUE)
rownames(merged_candidates)<-merged_candidates$Row.names
merged_candidates$Row.names <- NULL
merged_candidates<-merge(merged_candidates, df_fiorina, by="row.names", all=TRUE)


## fix NAs
merged_candidates[is.na(merged_candidates)]<-0

##Get rid of self references
merged_candidates["clinton", "clinton"]<-0
merged_candidates["trump", "trump"]<-0
merged_candidates["sanders", "sanders"]<-0
merged_candidates["fiorina", "fiorina"]<-0

## sum all
merged_candidates$all <- merged_candidates$trump + merged_candidates$clinton + merged_candidates$sanders + merged_candidates$fiorina

## sort it
merged_candidates<-merged_candidates[with(merged_candidates, order(-all)), ]

#merged_candidates <- merged_candidates[merged_candidates$all>50,]
## convert Row.names to a factor
merged_candidates$Row.names<-as.factor(merged_candidates$Row.names)

merged_candidates<-merged_candidates[complete.cases(merged_candidates),]

## This is what the data look like
#      Row.names trump sanders clinton fiorina all
# 1810    people    33      85      53      10 181
# 2483     think     9      55      90       9 163
# 1050     going    44      44      45      10 143
# 561    country    34      70      25       1 130
# 1363      know    23      26      56      19 124
# 2704      well     9      31      56       8 104

```

```{r, echo=FALSE, results="asis"}

library(xtable)

c_1<-merged_candidates[with(merged_candidates, order(-clinton)), ]
s_1<-merged_candidates[with(merged_candidates, order(-sanders)), ]
f_1<-merged_candidates[with(merged_candidates, order(-fiorina)), ]
t_1<-merged_candidates[with(merged_candidates, order(-trump)), ]

compare_mf <- rbind(c_1[1:4,], f_1[1:4,], s_1[1:4,], t_1[1:4,])
colnames(compare_mf)<-c("word", "trump", "sanders", "clinton", "fiorina", "sum")


print(xtable(unique(compare_mf), digits=0), type='html', comment=FALSE, include.rownames=FALSE, 
      html.table.attributes='border="3" align="center" ' )

```

  
Word count differ widely, reflecting the vocabulary choices made by each candidate. In addition, it's also apparent the number of words said by each candidate differed greatly due to the larger number of Republican candidates (~roughly ten) than Democratic one (~ roughly three). Indeed, the total number of words spoken by Carly Fiorina was `r words_fiorina` and her vocabularly of distinct words was `r vocab_fiorina`. By comparison, Bernie Sanders said `r words_sanders` total words, with a vocabulary of `r vocab_sanders` words.   

####GRAPHICAL REPRESENTATION

From the above, there may be information in comparing words used frequently by one candidate to frequency of use by another. Here is a graph of the "top" words used by all candidates. From the above we need to be careful to normalize the word count, 
$\nu_{i} = W_{i} / \sum_{k=1}^{N} W_{k}$, where $\nu_{i}$ is the normalized frequency of word $i$ with count $W_{i}$.


In the graph below the $\nu_{i}$ for each candidate are plotted for the most-used words as measured for the ensemble of all candidates. 

```{r, echo=FALSE, fig.align="center", fig.height= 5, fig.width=8, message=FALSE, warning=FALSE}

library(reshape2)

merged_plot<-melt(merged_candidates[merged_candidates$all>50,])
colnames(merged_plot)<-c("words", "candidate", "count")

library(ggplot2)

# p <- ggplot(merged_plot, aes(x = words, y = count, color=candidate))
# p <- p + geom_point(size=3, pch=19)
# p <- p + theme(axis.text.x=element_text(face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"))
# p <- p + coord_flip()
# p <- p + ggtitle("CANDIDATE WORD COUNT")
# p <- p + xlab("word")
# p <- p + ylab("count")
# p 

```
```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=8, message=FALSE, warning=FALSE}
mc<-merged_candidates
mc$rank <- 1:nrow(mc)
##normalize
mc$clinton<-mc$clinton/sum(mc$clinton)
mc$sanders<-mc$sanders/sum(mc$sanders)
mc$trump<-mc$trump/sum(mc$trump)
mc$fiorina<-mc$fiorina/sum(mc$fiorina)

mc_plot<-mc[mc$all>40,]

p <- ggplot(mc_plot, aes(x = rank, size=3))
#p <- p + geom_line(size=0.5)
p <- p + geom_point(aes(y = clinton, color="clinton"), size=3)
p <- p + geom_point(aes(y = sanders, color="sanders"), size=3)
p <- p + geom_point(aes(y = trump, color="trump"), size=3)
p <- p + geom_point(aes(y = fiorina, color="fiorina"), size=3)
p <- p + theme(axis.text.x=element_text(angle=90, hjust=1, face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"),legend.title=element_blank())
#p <- p + coord_flip()
p <- p + scale_x_discrete(breaks=1:length(mc$Row.names),
        labels=mc$Row.names)
p <- p + ggtitle("MOST-USED WORD FREQUENCIES")
p <- p + xlab("")
p <- p + ylab("normalized frequency")
p 

```

This starts to be much more informative. For instance, Carly Fiorina mentions the word _"government"_ as almost two percent of her word usage, whereas Donald Trump hardly mentions the word at all. Or notice that both Bernie Sanders and Donald Trump mention the word _"wall"_ more than their competitors while Bernie Sanders alone mentions the word _"street"_ with comparably high frequency.

####NORMALIZED Z STATISTICS

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=8, message=FALSE, warning=FALSE}

##calculate mean

mc_plot$mean <- rowSums(mc_plot[,c(2,3,4,5)])/4

## subtract
mc_plot$sanders<-(mc_plot$sanders - mc_plot$mean)
mc_plot$clinton<-(mc_plot$clinton - mc_plot$mean)
mc_plot$fiorina<-(mc_plot$fiorina - mc_plot$mean)
mc_plot$trump<-(mc_plot$trump - mc_plot$mean)

## calc squared dev
mc_plot$sdev<- sqrt( ((mc_plot$sanders)^2 + (mc_plot$clinton)^2 + (mc_plot$fiorina)^2 + (mc_plot$trump)^2)/(4-1))

mc_plot$sanders<-(mc_plot$sanders/mc_plot$sdev)
mc_plot$clinton<-(mc_plot$clinton/mc_plot$sdev)
mc_plot$fiorina<-(mc_plot$fiorina/mc_plot$sdev)
mc_plot$trump<-(mc_plot$trump/mc_plot$sdev)



# p <- ggplot(mc_plot, aes(x = rank, size=3))
# #p <- p + geom_line(size=0.5)
# p <- p + geom_point(aes(y = clinton, color="clinton"), size=3)
# p <- p + geom_point(aes(y = sanders, color="sanders"), size=3)
# p <- p + geom_point(aes(y = trump, color="trump"), size=3)
# p <- p + geom_point(aes(y = fiorina, color="fiorina"), size=3)
# p <- p + theme(axis.text.x=element_text(angle=90, hjust=1, face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"),legend.title=element_blank())
# #p <- p + coord_flip()
# p <- p + scale_x_discrete(breaks=1:length(mc$Row.names),
#         labels=mc$Row.names)
# p <- p + ggtitle("Z_STATISTICS MOST FREQUENT WORDS")
# p <- p + xlab("")
# p <- p + ylab("normalized frequency")
# p 

```

The above doesn't reveal much more information than the wordcloud analysis does. However, we can also pick some "key words" and sample for their frequency. For a first stab, let's try


```{r, echo = 6:7, fig.align="center", fig.height=4, fig.width=6, message=FALSE, warning=FALSE }

library(reshape2)

merged_plot<-melt(merged_candidates)
colnames(merged_plot)<-c("words", "candidate", "count")

key_words = c("tax", "government", "climate", "class", "wall", "street","terror", "economy", "immigrant", "america", "veteran", "drug", "health", "gun", "education", "bankruptcy", "money", "women", "war", "rights", "abortion", "violence")
merged_plot<-merged_plot[merged_plot$word %in% key_words, ]
merged_plot<-merged_plot[!merged_plot$candidate=="all",]




p <- ggplot(merged_plot, aes(x = words, y = count))
p <- p + facet_grid(.~candidate)
p <- p + geom_point(size=3, pch=19, color = "#228222")
p <- p + theme(axis.text.x=element_text(face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"))
p <- p + coord_flip()
p <- p + ggtitle("DIFFERENCES IN CANDIDATE WORD CHOICE")
p <- p + xlab("")
p <- p + ylab("word count")
p 



mc<-mc[mc$Row.names %in% key_words,]
mc$rank<-1:nrow(mc)

head(mc)

p <- ggplot(mc, aes(x = rank, size=3))
#p <- p + geom_line(size=0.5)
p <- p + geom_point(aes(y = clinton, color="clinton"), size=3)
p <- p + geom_point(aes(y = sanders, color="sanders"), size=3)
p <- p + geom_point(aes(y = trump, color="trump"), size=3)
p <- p + geom_point(aes(y = fiorina, color="fiorina"), size=3)
p <- p + theme(axis.text.x=element_text(angle=90, hjust=1,face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"),legend.title=element_blank())
#p <- p + coord_flip()
p <- p + scale_x_discrete(breaks=1:25,
        labels=mc$Row.names)
p <- p + ggtitle("CANDIDATE WORD COUNT")
p <- p + xlab("")
p <- p + ylab("normalized frequency")
p 

```

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=8, message=FALSE, warning=FALSE}

##calculate mean

mc_plot<-mc

mc_plot$mean <- rowSums(mc_plot[,c(2,3,4,5)])/4

## subtract
mc_plot$sanders<-(mc_plot$sanders - mc_plot$mean)
mc_plot$clinton<-(mc_plot$clinton - mc_plot$mean)
mc_plot$fiorina<-(mc_plot$fiorina - mc_plot$mean)
mc_plot$trump<-(mc_plot$trump - mc_plot$mean)

## calc squared dev
mc_plot$sdev<- sqrt( ((mc_plot$sanders)^2 + (mc_plot$clinton)^2 + (mc_plot$fiorina)^2 + (mc_plot$trump)^2)/(4-1))

mc_plot$sanders<-(mc_plot$sanders/mc_plot$sdev)
mc_plot$clinton<-(mc_plot$clinton/mc_plot$sdev)
mc_plot$fiorina<-(mc_plot$fiorina/mc_plot$sdev)
mc_plot$trump<-(mc_plot$trump/mc_plot$sdev)



p <- ggplot(mc_plot, aes(x = rank, size=3))
#p <- p + geom_line(size=0.5)
p <- p + geom_point(aes(y = clinton, color="clinton"), size=3)
p <- p + geom_point(aes(y = sanders, color="sanders"), size=3)
p <- p + geom_point(aes(y = trump, color="trump"), size=3)
p <- p + geom_point(aes(y = fiorina, color="fiorina"), size=3)
p <- p + theme(axis.text.x=element_text(angle=90, hjust=1, face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"),legend.title=element_blank())
#p <- p + coord_flip()
p <- p + scale_x_discrete(breaks=1:length(mc$Row.names),
        labels=mc$Row.names)
p <- p + ggtitle("Z_STATISTICS MOST FREQUENT WORDS")
p <- p + xlab("")
p <- p + ylab("normalized frequency")
p 

```

###WORD ASSOCIATIONS FROM BIGRAM TOKENIZATION

Since word fequency does not convey specific positions on issues, let's look at word associations to see if we can get closer to meaning from more information about the context of word usage. This analysis simply tokenizes the text as bigrams, then uses a simple function

```{r eval=FALSE}
bigram_table[grep(word, rownames(bigram_table), ignore.case=TRUE)]
```

to pull out relevant terms from the torkenized TDM.
A key challenge is that the texts are relatively short, so statistics comparing the word frequencies are poor. Nevertheless, we can see that context around different words, even at the relatively unsophisticated level of simple bigrams, starts to hint at differences in approach to problems. 

```{r, echo=FALSE}

library(RWeka)
bigramTokenizer <- function(x) NGramTokenizer(x, 
                                Weka_control(min = 2, max = 2))

trump_bigrams <- sort(table(bigramTokenizer(trump_all)), decreasing = TRUE)
sanders_bigrams <- sort(table(bigramTokenizer(sanders_all)), decreasing = TRUE)
clinton_bigrams <- sort(table(bigramTokenizer(clinton_all)), decreasing = TRUE)
fiorina_bigrams <- sort(table(bigramTokenizer(fiorina_all)), decreasing = TRUE)

bigram_finder <- function(word, bigram_table){
    relevant_bigrams <- bigram_table[grep(word, rownames(bigram_table), ignore.case=TRUE)]
    return(relevant_bigrams)
}

```



###CANDIDATES ON TAXES

Bernie talks about _"tax"_ and _"terror"_ as well. His discussion of taxes has a reformist bent, but where Carly Fiorina talks associates words like budgeting, changes, reform, simplify, code, reform, and plan, Bernie Sanders associates words like cap, income, must, share, speculation, breaks, reform, wall, and rebuilding. 




```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=6, warning=FALSE, message=FALSE}


word_sample <- "tax"
a<-bigram_finder(word_sample, fiorina_bigrams)
a<-as.data.frame(a)
colnames(a)<-word_sample

bigram_freq<-a

library(ggplot2)

p <- ggplot(bigram_freq, aes(x=rownames(bigram_freq), y = bigram_freq[,1]))
p <- p + geom_bar(fill="#751B1C", stat="identity")
p <- p + ggtitle(paste0("FIORINA bigram sample: \"", word_sample,"\" "))
p <- p + xlab("bigrams")
p <- p + ylab("count")
p <- p + theme(axis.text.x=element_text(face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"))
p <- p + scale_y_continuous(limits=c(0, 10), breaks=c(0, 2, 4, 6, 8, 10))
p <- p + coord_flip()
plot_tax_fiorina <- p
p

```

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=6, warning=FALSE, message=FALSE}


#word_sample <- "tax"
a<-bigram_finder(word_sample, sanders_bigrams)
a<-as.data.frame(a)
colnames(a)<-word_sample

bigram_freq<-a

library(ggplot2)

p <- ggplot(bigram_freq, aes(x=rownames(bigram_freq), y = bigram_freq[,1]))
p <- p + geom_bar(fill="#415DA3", stat="identity")
p <- p + ggtitle(paste0("SANDERS bigram sample: \"", word_sample,"\" "))
p <- p + xlab("bigrams")
p <- p + ylab("count")
p <- p + theme(axis.text.x=element_text(face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"))
p <- p + scale_y_continuous(limits=c(0, 10), breaks=c(0,2,4,6,8,10))
p <- p + coord_flip()
plot_tax_sanders <- p

plot_tax_sanders

```

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=6, warning=FALSE, message=FALSE}


#word_sample <- "tax"
a<-bigram_finder(word_sample, trump_bigrams)
a<-as.data.frame(a)
colnames(a)<-word_sample

bigram_freq<-a

library(ggplot2)

p <- ggplot(bigram_freq, aes(x=rownames(bigram_freq), y = bigram_freq[,1]))
p <- p + geom_bar(fill="#751B1C", stat="identity")
p <- p + ggtitle(paste0("TRUMP bigram sample: \"", word_sample,"\" "))
p <- p + xlab("bigrams")
p <- p + ylab("count")
p <- p + theme(axis.text.x=element_text(face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"))
p <- p + scale_y_continuous(limits=c(0, 10), breaks=c(0,2,4,6,8,10))
p <- p + coord_flip()
p

```

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=6, warning=FALSE, message=FALSE}


#word_sample <- "tax"
a<-bigram_finder(word_sample, clinton_bigrams)
a<-as.data.frame(a)
colnames(a)<-word_sample

bigram_freq<-a

library(ggplot2)

p <- ggplot(bigram_freq, aes(x=rownames(bigram_freq), y = bigram_freq[,1]))
p <- p + geom_bar(fill="#415DA3", stat="identity")
p <- p + ggtitle(paste0("CLINTON bigram sample: \"", word_sample,"\" "))
p <- p + xlab("bigrams")
p <- p + ylab("count")
p <- p + theme(axis.text.x=element_text(face="italic", color="#222222"), axis.text.y=element_text(face="italic", color="#222222"))
p <- p + scale_y_continuous(limits=c(0, 10), breaks=c(0,2,4,6,8,10))
p <- p + coord_flip()
p



```

